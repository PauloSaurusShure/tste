# Use the official CUDA image as the base
FROM nvidia/cuda:11.8.0-cudnn8-runtime-ubuntu20.04 as cuda_base

# Set noninteractive mode
ENV DEBIAN_FRONTEND noninteractive

# Install necessary dependencies including CMake and g++
RUN apt-get update && apt-get install -y \
    libopenblas-dev \
    ninja-build \
    build-essential \
    pkg-config \
    wget \
    make \
    cmake \ 
    g++ \ 
    gcc 

# Switch to the Python image for the final build stage
FROM python:3.11.6-slim-bookworm as base

# Install poetry
RUN pip install pipx
RUN python3 -m pipx ensurepath
RUN pipx install poetry
ENV PATH="/root/.local/bin:$PATH"
ENV PATH=".venv/bin/:$PATH"

# Copy CUDA dependencies from the CUDA base image
COPY --from=cuda_base /usr/local/cuda /usr/local/cuda

# Set environment variables for CUDA
ENV PATH="/usr/local/cuda/bin:${PATH}"
ENV LD_LIBRARY_PATH="/usr/local/cuda/lib64:${LD_LIBRARY_PATH}"

# https://python-poetry.org/docs/configuration/#virtualenvsin-project
ENV POETRY_VIRTUALENVS_IN_PROJECT=true

# Continue with the rest of your Dockerfile
FROM base as dependencies
WORKDIR /home/worker/app
COPY pyproject.toml poetry.lock ./

# Install dependencies with CUDA support
ENV CMAKE_ARGS="-DLLAMA_CUBLAS=on"
RUN poetry install --with local
RUN poetry install --with ui

# Install llama-cpp-python with CUDA support
RUN poetry run pip install --force-reinstall --no-cache-dir llama-cpp-python
RUN poetry run python scripts/setup

FROM base as app

ENV PYTHONUNBUFFERED=1
ENV PORT=8080
EXPOSE 8080

# Prepare a non-root user
RUN adduser --system worker
WORKDIR /home/worker/app

RUN mkdir local_data; chown worker local_data
RUN mkdir models; chown worker models
COPY --chown=worker --from=dependencies /home/worker/app/.venv/ .venv
COPY --chown=worker private_gpt/ private_gpt
COPY --chown=worker fern/ fern
COPY --chown=worker *.yaml *.md ./
COPY --chown=worker scripts/ scripts

ENV PYTHONPATH="$PYTHONPATH:/private_gpt/"

USER worker
ENTRYPOINT python -m private_gpt
